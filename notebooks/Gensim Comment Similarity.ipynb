{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Similarity via Doc2Vec (`gensim`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import gensim\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training and test data\n",
    "Load `comments.csv` file as it contains our crawled raw data. It is in cvs-format and contains one column `comment text`, which contain the comment text and therefore form our corpus. Each document gets a label assigned. The label is the respective comment ID (`comment id`). \n",
    "\n",
    "Preprocess data with `gensim` (resulting in a list of words) and split to train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        # setup csv reader\n",
    "        csv.field_size_limit(1000000000)\n",
    "        self.n_all_samples = 0\n",
    "        self.train_indices = []\n",
    "        self.test_indices = []\n",
    "        self._is_splitting_setup = False\n",
    "        \n",
    "    def init_splitting(self, test_samples=0.2, random_seed=15):\n",
    "        from numpy.random import RandomState\n",
    "        \n",
    "        # count corpus size\n",
    "        with open(self.filename, 'rt', encoding=\"UTF-8\", newline=\"\") as file:\n",
    "            for i, _ in enumerate(file, 1):\n",
    "                pass\n",
    "        self.n_all_samples = i\n",
    "        # split into train and test\n",
    "        random = RandomState(random_seed)\n",
    "        n_samples = int(self.n_all_samples*test_samples)\n",
    "        all_indices = set(range(0, self.n_all_samples))\n",
    "        self.test_indices = set(random.randint(0, n_samples, size=(self.n_all_samples,)))\n",
    "        self.train_indices = all_indices - self.test_indices\n",
    "        self._is_splitting_setup = True\n",
    "        print(\"corpus size:\", self.n_all_samples)\n",
    "        print(\"train corpus size:\", len(self.train_indices))\n",
    "        print(\"test corpus size:\", len(self.test_indices))\n",
    "    \n",
    "    def save(self, filename):\n",
    "        if not self._is_splitting_setup:\n",
    "            raise IllegalArgumentException(\"No split indices available to save to disk\")\n",
    "        with open(filename, 'wt', encoding=\"UTF-8\", newline=\"\") as file:\n",
    "            writer = csv.writer(file, delimiter=\",\")\n",
    "            writer.writerow(self.train_indices)\n",
    "            writer.writerow(self.test_indices)\n",
    "        print(\"Wrote indices to disk:\", filename)\n",
    "    \n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rt', encoding=\"UTF-8\", newline=\"\") as file:\n",
    "            reader = csv.reader(file, delimiter=\",\")\n",
    "            self.train_indices = next(reader)\n",
    "            self.test_indices = next(reader)\n",
    "        self.n_all_samples = len(self.train_indices) + len(self.test_indices)\n",
    "        print(\"Loaded indices from disk:\", filename)\n",
    "    \n",
    "    def read_corpus_with_indices(self, indices):\n",
    "        with open(self.filename, 'rt', encoding=\"UTF-8\", newline=\"\") as file:\n",
    "            reader = csv.reader(file, delimiter=\",\")\n",
    "            next(reader) #skipping header\n",
    "            for i, row in enumerate(reader, 0):\n",
    "                if not self._is_splitting_setup:\n",
    "                    yield gensim.models.doc2vec.TaggedDocument(\n",
    "                        gensim.utils.simple_preprocess(row[3].replace(\"\\\\n\", \" \")), \n",
    "                        [int(row[0])]\n",
    "                    )\n",
    "                else:\n",
    "                    if i in indices:\n",
    "                        yield gensim.models.doc2vec.TaggedDocument(\n",
    "                            gensim.utils.simple_preprocess(row[3].replace(\"\\\\n\", \" \")), \n",
    "                            [int(row[0])]\n",
    "                        )\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "    def read_corpus_with_id(self, cid):\n",
    "        with open(self.filename, 'rt', encoding=\"UTF-8\", newline=\"\") as file:\n",
    "            reader = csv.reader(file, delimiter=\",\")\n",
    "            next(reader) #skipping header\n",
    "            for i, row in enumerate(reader, 0):\n",
    "                if int(row[0]) in cid:\n",
    "                    yield gensim.models.doc2vec.TaggedDocument(\n",
    "                        gensim.utils.simple_preprocess(row[3].replace(\"\\\\n\", \" \")), \n",
    "                        [int(row[0])]\n",
    "                    )\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.read_corpus_with_indices(self.train_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_source = os.path.join(\"..\", \"data\", \"comments.data\")\n",
    "filename_model = \"word2vec_2.model\"\n",
    "filename_indices = \"indices_2.model\"\n",
    "size = 50\n",
    "min_count = 10\n",
    "iterations = 10\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter:\n",
    "- **`size` layer size**\n",
    "  \n",
    "  This parameter is the size of the NN layers, which correspond to the “degrees” of freedom the training algorithm has. Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds.\n",
    "  \n",
    "- **`min_count` only use words that occur min. two times:**\n",
    "  \n",
    "  It's for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them.\n",
    "\n",
    "- **`iter` number of training iterations on the dataset**\n",
    "  \n",
    "- **`workers` number of parallel worker threats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 5608810\n",
      "train corpus size: 5328370\n",
      "test corpus size: 280440\n",
      "Building vocabulary\n",
      "Wall time: 5min 5s\n",
      "Training model\n",
      "Wall time: 1h 16min 16s\n",
      "Wrote indices to disk: indices_2.model\n",
      "Model saved to disk; Pruned in-memory model\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(filename_source)\n",
    "preprocessor.init_splitting(test_samples=0.05, random_seed=15)\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(size=size, min_count=min_count, iter=iterations, workers=workers)\n",
    "print(\"Building vocabulary\")\n",
    "%time model.build_vocab(preprocessor)\n",
    "print(\"Training model\")\n",
    "%time model.train(preprocessor, total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "# save model to disk\n",
    "model.save(filename_model)\n",
    "preprocessor.save(filename_indices)\n",
    "# trim memory usage:\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "print(\"Model saved to disk; Pruned in-memory model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44385. Test Document (44385): «thanks ed another few thousand yes votes now could we have speech about new tartan curtain tearing loving families apart forever cheers»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc10,s0.001,t4):\n",
      "\n",
      "MOST (3960056, 0.751582145690918): «sorry but with corbyn as labour leader there is no opposition party many thanks to corbyn supporters for allowing the tories free run at the next ge the many homeless disabled and vulnerable people will thank you for it»\n",
      "\n",
      "SECOND MOST (447479, 0.7420821189880371): «comrade corbyn and ira lover mcdonnel are the lords of misrule as labour continues to disintegrate please god let them labour to defeat in»\n",
      "\n",
      "MEDIAN (4438348, 0.35840940475463867): «negotiate from position of strength one can only imagine the excuses people would make as eu would take more from there sovereign nations without the say of the people to stop them oh wait we can greece spain portugal ireland feel the power of vote you loose that or degrade it in any way you doom the future by giving control to group of elites»\n",
      "\n",
      "LEAST (169346, -0.6046342253684998): «come now dodgers my childhood was in the but hell will freeze over before become kipper»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "#model = gensim.models.doc2vec.Doc2Vec.load(filename_model)\n",
    "# Load preprocessor\n",
    "#preprocessor = Preprocessor(filename_source)\n",
    "#preprocessor.load(filename_indices)\n",
    "\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_idx = random.choice(list(preprocessor.test_indices))\n",
    "doc = next(preprocessor.read_corpus_with_indices([doc_idx]))\n",
    "inferred_vector = model.infer_vector(doc.words)\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('{}. Test Document ({}): «{}»\\n'.format(doc_idx, doc.tags[0], ' '.join(doc.words)))\n",
    "print('SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print('%s %s: «%s»\\n' % (label, sims[index], ' '.join(next(preprocessor.read_corpus_with_id([sims[index][0]])).words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('america', 0.6986275911331177),\n",
       " ('china', 0.6789929270744324),\n",
       " ('germany', 0.6717723608016968),\n",
       " ('mexico', 0.6707333326339722),\n",
       " ('congress', 0.6639487743377686),\n",
       " ('japan', 0.6501384377479553),\n",
       " ('uk', 0.6442731022834778),\n",
       " ('us', 0.6404613256454468),\n",
       " ('tpp', 0.6269809007644653),\n",
       " ('france', 0.6255526542663574)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"trump\", \"usa\"], negative=[\"putin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity trump - putin: 0.8540221587833772\n",
      "similarity trump - clinton: 0.7710283588666192\n",
      "similarity trump - president: 0.5690041895616303\n",
      "similarity president - safety: 0.013856460909897117\n",
      "similarity election - fun: 0.1425054690336343\n",
      "similarity election - but: 0.365167402266884\n"
     ]
    }
   ],
   "source": [
    "similarities = [\n",
    "    (\"trump\", \"putin\"),\n",
    "    (\"trump\", \"clinton\"),\n",
    "    (\"trump\", \"president\"),\n",
    "    (\"president\", \"safety\"),\n",
    "    (\"election\", \"fun\"),\n",
    "    (\"election\", \"but\")\n",
    "]\n",
    "for w1, w2 in similarities:\n",
    "    print(\"similarity {} - {}: {}\".format(w1, w2, model.wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ebola', 0.6875036954879761),\n",
       " ('infection', 0.6856157779693604),\n",
       " ('desease', 0.6590758562088013),\n",
       " ('diseases', 0.6472394466400146),\n",
       " ('obesity', 0.6398852467536926),\n",
       " ('cancer', 0.6353930234909058),\n",
       " ('infections', 0.6333891153335571),\n",
       " ('illness', 0.628749668598175),\n",
       " ('tb', 0.6269285082817078),\n",
       " ('alcoholism', 0.612775444984436)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"malaria\", \"disease\"], negative=[\"penicillin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
